{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import sys\n",
    "\n",
    "%run \"common_functions.py\"\n",
    "\n",
    "sys.setrecursionlimit(53000) # override needed for computing midpoints, which uses a recursive function\n",
    "Image.MAX_IMAGE_PIXELS = 366498276 # override is needed, or else it gives a DecompressionBombError\n",
    "\n",
    "cy1_shifted_file = \"originals/shiftedcycle 1thnksgv.tif\"\n",
    "cy2_shifted_file = \"originals/shiftedcycle 2thnksgv.tif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c(img):\n",
    "    return Image.fromarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_to_align = 2\n",
    "\n",
    "img_cy1 = Image.open(cy1_shifted_file)\n",
    "brightfield_cy1 = np.array(img_cy1)\n",
    "brightfield_cy1 = (brightfield_cy1/256).astype('uint8') # don't need the whole range of values, so this reduces mem size, + improves access speed\n",
    "\n",
    "img_cy2 = Image.open(cy2_shifted_file)\n",
    "brightfield_cy2 = np.array(img_cy2)\n",
    "brightfield_cy2 = (brightfield_cy2/256).astype('uint8') \n",
    "\n",
    "img_cy1 = None\n",
    "img_cy2 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y,height = 0, 2500\n",
    "# x, width = 0, 2500\n",
    "# img1 = brightfield_cy1[y:height, x:width]\n",
    "# img2 = brightfield_cy2[y:height, x:width]\n",
    "\n",
    "img1 = brightfield_cy1\n",
    "img2 = brightfield_cy2\n",
    "\n",
    "# brightfield_cy1 = None\n",
    "# brightfield_cy2 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cutimage(img):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignImages(im1, im2):\n",
    "  # Convert images to grayscale\n",
    "  #im1Gray = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)\n",
    "  #im2Gray = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
    " \n",
    "  # Detect ORB features and compute descriptors.\n",
    "  orb = cv2.ORB_create(10000)\n",
    "  keypoints1, descriptors1 = orb.detectAndCompute(im1, None)\n",
    "  keypoints2, descriptors2 = orb.detectAndCompute(im2, None)\n",
    " \n",
    "  # Match features.\n",
    "  matcher = cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING)\n",
    "  matches = list(matcher.match(descriptors1, descriptors2, None))\n",
    " \n",
    "  # Sort matches by score\n",
    "  matches.sort(key=lambda x: x.distance, reverse=False)\n",
    " \n",
    "  # Remove not so good matches\n",
    "  print(\"Count of Matches:\", len(matches))\n",
    "  numGoodMatches = int(len(matches) * .05)\n",
    "  print(\"Count of Good Matches:\", numGoodMatches)\n",
    "  print(\"Distance of worst match:\", matches[numGoodMatches].distance)\n",
    "  matches = matches[:numGoodMatches]\n",
    " \n",
    "  # Draw top matches\n",
    "  #imMatches = cv2.drawMatches(im1, keypoints1, im2, keypoints2, matches, None)\n",
    "  #cv2.imwrite(\"matches.jpg\", imMatches)\n",
    " \n",
    "  # Extract location of good matches\n",
    "  points1 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "  points2 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    " \n",
    "  for i, match in enumerate(matches):\n",
    "    points1[i, :] = keypoints1[match.queryIdx].pt\n",
    "    points2[i, :] = keypoints2[match.trainIdx].pt\n",
    " \n",
    "  # Find homography\n",
    "  h, mask = cv2.findHomography(points1, points2, cv2.RANSAC)\n",
    " \n",
    "  # Use homography\n",
    "  height, width = im2.shape\n",
    "  im1Reg = cv2.warpPerspective(im1, h, (width, height))\n",
    " \n",
    "  return im1Reg, h\n",
    " \n",
    "num_alignments = 1\n",
    "alignable = adjust_contrast(img1.copy())\n",
    "print(alignable.shape)\n",
    "\n",
    "# crop along boundaries\n",
    "alignable[img2 == 0] = 0\n",
    "img2[alignable == 0] = 0\n",
    "\n",
    "for align_n in range(num_alignments):\n",
    "  alignable, h = alignImages(alignable,adjust_contrast(img2))\n",
    "\n",
    "c(adjust_contrast(img2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gitkp(im1, im2):\n",
    "  # Convert images to grayscale\n",
    "  #im1Gray = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)\n",
    "  #im2Gray = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
    " \n",
    "  # Detect ORB features and compute descriptors.\n",
    "  orb = cv2.ORB_create(100000)\n",
    "  keypoints1, descriptors1 = orb.detectAndCompute(im1, None)\n",
    "  keypoints2, descriptors2 = orb.detectAndCompute(im2, None)\n",
    " \n",
    "  # Match features.\n",
    "  matcher = cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING)\n",
    "  matches = list(matcher.match(descriptors1, descriptors2, None))\n",
    " \n",
    "  # Sort matches by score\n",
    "  matches.sort(key=lambda x: x.distance, reverse=False)\n",
    " \n",
    "  # Remove not so good matches\n",
    "  print(\"Count of Matches:\", len(matches))\n",
    "  numGoodMatches = int(len(matches) * .5)\n",
    "  print(\"Count of Good Matches:\", numGoodMatches)\n",
    "  print(\"Distance of worst match:\", matches[numGoodMatches].distance)\n",
    "  matches = matches[:numGoodMatches]\n",
    " \n",
    "  # Draw top matches\n",
    "  #imMatches = cv2.drawMatches(im1, keypoints1, im2, keypoints2, matches, None)\n",
    "  #cv2.imwrite(\"matches.jpg\", imMatches)\n",
    " \n",
    "  # Extract location of good matches\n",
    "  points1 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "  points2 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    " \n",
    "  for i, match in enumerate(matches):\n",
    "    points1[i, :] = keypoints1[match.queryIdx].pt\n",
    "    points2[i, :] = keypoints2[match.trainIdx].pt\n",
    " \n",
    "  imageresult1 = cv2.drawKeypoints(im1, keypoints1, None, color=(255,0,0), flags=0)\n",
    "  imageresult2 = cv2.drawKeypoints(im2, keypoints2, None, color=(255,0,0), flags=0)\n",
    "  return imageresult1, imageresult2\n",
    " \n",
    "# imageresult1, imageresult2 = gitkp(adjust_contrast(img1),adjust_contrast(img2))\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c(imageresult1).save(\"kp1.png\")\n",
    "c(imageresult2).save(\"kp2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c(imageresult1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c(alignable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c(alignable).save(\"cunty.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/make-knn-300-times-faster-than-scikit-learns-in-20-lines-5e29d74e76bb\n",
    "https://medium.com/aiguys/other-versions-of-knn-fast-nearest-neighbors-43a051bfb9aa\n",
    "https://stackoverflow.com/questions/51688568/faster-knn-classification-algorithm-in-python#:~:text=KNN%20is%20a%20very%20slow,all%20the%20distances%20and%20sorting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_contrast(img, min=2, max = 98):\n",
    "    # pixvals = np.array(img)\n",
    "\n",
    "    minval = np.percentile(img, min) # room for experimentation \n",
    "    maxval = np.percentile(img, max) # room for experimentation \n",
    "    img = np.clip(img, minval, maxval)\n",
    "    img = ((img - minval) / (maxval - minval)) * 255\n",
    "    return (img.astype(np.uint8))\n",
    "\n",
    "def threshold(im, percentile):\n",
    "    p = np.percentile(im, percentile)\n",
    "    im[im < p]  = 0\n",
    "    im[im >= p]  = 255\n",
    "    \n",
    "    return im\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = {\n",
    "    0: \"only neither is  solid\", \n",
    "    1: \"only cycle 1 is solid\",\n",
    "    2: \"only cycle 2 is solid\", \n",
    "    3: \"both are solid\",     \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c(alignable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c(threshold(adjust_contrast(alignable), 90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def color_in(mat_orig, min_size=0):\n",
    "    mat = (mat_orig)\n",
    "    gray_edit_copy = mat_orig.copy()\n",
    "\n",
    "    def fill_blob(x, y):\n",
    "        \n",
    "        mat[y][x] = False\n",
    "        gray_edit_copy[y][x] = False\n",
    "        \n",
    "        #  right\n",
    "        if  (x != mat.shape[1] - 1) and mat[y][x+1]:\n",
    "            (fill_blob(x+1, y))\n",
    "        # down\n",
    "        if  (y != mat.shape[0] - 1) and mat[y+1][x]:\n",
    "            (fill_blob(x, y+1))\n",
    "        # left\n",
    "        if  (x != 0) and mat[y][x-1]:\n",
    "            (fill_blob(x-1, y))\n",
    "        # up\n",
    "        if  (y != 0) and  mat[y-1][x]:\n",
    "            fill_blob(x, y-1)\n",
    "        \n",
    "    for y, line in enumerate(mat):\n",
    "        for x, pixel in enumerate(line):\n",
    "            if pixel == 3:\n",
    "                fill_blob(x, y)\n",
    "                \n",
    "    \n",
    "    return gray_edit_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "im1_beads = threshold(adjust_contrast(alignable), 90)\n",
    "im2_beads = (threshold(adjust_contrast(img2), 90))\n",
    "\n",
    "c(im1_beads)\n",
    "\n",
    "# im1_beads[im1\n",
    "# _beads == 255] = 1\n",
    "# im2_beads[im2_beads == 255] = 2\n",
    "\n",
    "# overlaid = np.add(im1_beads, im2_beads)\n",
    "\n",
    "# c((overlaid)*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "im1_beads = threshold(adjust_contrast(alignable), 85)\n",
    "im2_beads = (threshold(adjust_contrast(img2), 85))\n",
    "\n",
    "im1_beads[im1_beads == 255] = 1\n",
    "im2_beads[im2_beads == 255] = 2\n",
    "\n",
    "overlaid = np.add(im1_beads, im2_beads)\n",
    "\n",
    "\n",
    "# c(adjust_contrast(color_in(overlaid), 0, 100))\n",
    "# c(im2_beads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlaid_disjoint = color_in(overlaid)\n",
    "c(overlaid_disjoint * 100).save('overlaid_disjoint.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4 / 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_blob(mat_orig, xy):\n",
    "    mat = (mat_orig != 0)\n",
    "    queue = mat_orig.copy()\n",
    "    def fill_blob(x, y):\n",
    "        queue[y][x] = 0\n",
    "        mat[y][x] = False\n",
    "        blob_elems = [[x, y]]\n",
    "        #  right\n",
    "        if  (x != mat.shape[1] - 1) and mat[y][x+1]:\n",
    "            blob_elems += (fill_blob(x+1, y))\n",
    "        # down\n",
    "        if  (y != mat.shape[0] - 1) and mat[y+1][x]:\n",
    "            blob_elems +=(fill_blob(x, y+1))\n",
    "        # left\n",
    "        if  (x != 0) and mat[y][x-1]:\n",
    "            blob_elems +=(fill_blob(x-1, y))\n",
    "        # up\n",
    "        if  (y != 0) and  mat[y-1][x]:\n",
    "            blob_elems +=(fill_blob(x, y-1))\n",
    "        \n",
    "        \n",
    "        return blob_elems\n",
    "    \n",
    "    fill_blob(xy[0], xy[1])\n",
    "    \n",
    "    return queue\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import diplib as dip\n",
    "\n",
    "# out = dip.AreaOpening(imm_cy1, filterSize=3, connectivity=1)\n",
    "# c(np.array(out))\n",
    "\n",
    "%run \"common_functions.py\"\n",
    "\n",
    "midpoints_cy1, sizes_cy1, gray_edit_copy_cy1, filter1 =  get_center_points(imm_cy1, 5, filter=True, gray=False)\n",
    "midpoints_cy2, sizes_cy2, gray_edit_copy_cy2, filter2 =  get_center_points(imm_cy2, 5, filter=True, gray=False)\n",
    "\n",
    "filter1[filter1 != 255] = 0\n",
    "filter2[filter2 != 255] = 0\n",
    "\n",
    "filter1[filter1 == 255] = 1\n",
    "filter2[filter2 == 255] = 2\n",
    "\n",
    "overlaid_filtered = np.add(filter1, filter2)\n",
    "\n",
    "# filter1[filter1 == 1] = 255           \n",
    "# filter2[filter2 == 2] = 255\n",
    "\n",
    "print(len(midpoints_cy1))\n",
    "print(len(midpoints_cy2))\n",
    "\n",
    "# c(filter2*100)\n",
    "c(np.add(filter1, filter2) * 100).save('cc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def closest_node(node, nodes):\n",
    "    return nodes[cdist([node], nodes).argmin()]\n",
    "# print(midpoints_cy2)\n",
    "\n",
    "lines_between = (overlaid_filtered.copy()) * 100\n",
    "\n",
    "\n",
    "\n",
    "if len(lines_between.shape) == 2:\n",
    "        lines_between = np.stack((lines_between,)*3, axis=2)\n",
    "\n",
    "cleaned_im1 = filter1.copy()\n",
    "cleaned_im2 = filter2.copy()\n",
    "\n",
    "import math\n",
    "DISTANCE_THRESHOLD = 10\n",
    "\n",
    "for i, mp in  enumerate(midpoints_cy2):\n",
    "    cn = closest_node(mp, midpoints_cy1)\n",
    "    # print(i)\n",
    "    # print(math.dist(mp, cn))\n",
    "    if math.dist(mp, cn) < DISTANCE_THRESHOLD:\n",
    "        cleaned_im2 = remove_blob(cleaned_im2, mp)\n",
    "        cleaned_im1 = remove_blob(cleaned_im1, cn)\n",
    "        \n",
    "    #     lines_between = cv2.line(lines_between, mp, cn, (0, 255, 0), 1) \n",
    "    # else:\n",
    "    #     lines_between = cv2.line(lines_between, mp, cn, (255, 0, 0), 1) \n",
    "\n",
    "    # if i == 200:\n",
    "    #     break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c(lines_between)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c(cleaned_im1*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.opencv.org/3.4/d2/d2c/tutorial_sobel_derivatives.html\n",
    "grad_x = cv.Sobel(adjust_contrast(im1_r_onto_im2_third4), cv.CV_16S, 1, 0, ksize=3, scale=1, delta=0, borderType=cv.BORDER_DEFAULT)\n",
    "    # Gradient-Y\n",
    "    # grad_y = cv.Scharr(gray,ddepth,0,1)\n",
    "grad_y = cv.Sobel(adjust_contrast(im1_r_onto_im2_third4), cv.CV_16S, 0, 1, ksize=3, scale=1, delta=0, borderType=cv.BORDER_DEFAULT)\n",
    "\n",
    "abs_grad_x = cv.convertScaleAbs(grad_x)\n",
    "abs_grad_y = cv.convertScaleAbs(grad_y)\n",
    "    \n",
    "    ``\n",
    "grad = cv.addWeighted(abs_grad_x, 0.5, abs_grad_y, 0.5, 0)\n",
    "    \n",
    "num = 200\n",
    "# c(adjust_contrast(grad))\n",
    "grad[grad >= num] = 255\n",
    "grad[grad < num] = 0\n",
    "c(grad)\n",
    "# c(threshold(adjust_contrast(adjust_contrast(im1_r_onto_im2_third4)), 80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddepth = cv.CV_16S\n",
    "kernel_size = 3\n",
    "window_name = \"Laplace Demo\"\n",
    "\n",
    "src = im1_r_onto_im2_third4.copy()\n",
    "src = adjust_contrast(src)\n",
    "\n",
    "\n",
    "dst = cv.Laplacian(src, ddepth, ksize=kernel_size)\n",
    "# [laplacian]\n",
    "# [convert]\n",
    "# converting back to uint8\n",
    "abs_dst = cv.convertScaleAbs(dst)\n",
    "\n",
    "\n",
    "def blur_noise_reduction(im):\n",
    "\n",
    "\n",
    "    im1 = im1.filter(ImageFilter.BLUR)\n",
    "    # im1 = im1.filter(ImageFilter.BLUR)\n",
    "    # im1 = im1.filter(ImageFilter.BLUR)\n",
    "    # im1 = im1.filter(ImageFilter.BLUR)\n",
    "    # im1 = im1.filter(ImageFilter.BLUR)\n",
    "    # im1 = im1.filter(ImageFilter.BLUR)\n",
    "    # im1 = im1.filter(ImageFilter.BLUR).filter(ImageFilter.BLUR).filter(ImageFilter.BLUR).filter(ImageFilter.BLUR).filter(ImageFilter.BLUR).filter(ImageFilter.BLUR)\n",
    "    # im1 = im1 - np.array(diplib.AreaOpening(np.array(im1), filterSize=25, connectivity=2))\n",
    "    \n",
    "\n",
    "\n",
    "    return im1\n",
    "\n",
    "from PIL import ImageFilter\n",
    "# c(threshold(adjust_contrast(np.array(c(abs_dst).filter(ImageFilter.BLUR))), 80))\n",
    "c((abs_dst))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "flow = cv.calcOpticalFlowFarneback(np.array(im1_r_onto_im2_third4), \n",
    "                                   np.array(img2), \n",
    "                                   None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "def put_optical_flow_arrows_on_image(image, optical_flow_image, threshold=2.0, skip_amount=30):\n",
    "    # Don't affect original image\n",
    "    image = image.copy()\n",
    "    \n",
    "    # Turn grayscale to rgb if needed\n",
    "    if len(image.shape) == 2:\n",
    "        image = np.stack((image,)*3, axis=2)\n",
    "    \n",
    "    # Get start and end coordinates of the optical flow\n",
    "    flow_start = np.stack(np.meshgrid(range(optical_flow_image.shape[1]), range(optical_flow_image.shape[0])), 2)\n",
    "    flow_end = (optical_flow_image[flow_start[:,:,1],flow_start[:,:,0],:1]*3 + flow_start).astype(np.int32)\n",
    "    \n",
    "\n",
    "    # Threshold values\n",
    "    norm = np.linalg.norm(flow_end - flow_start, axis=2)\n",
    "    norm[norm < threshold] = 0\n",
    "    \n",
    "    # Draw all the nonzero values\n",
    "    nz = np.nonzero(norm)\n",
    "    for i in range(0, len(nz[0]), skip_amount):\n",
    "        y, x = nz[0][i], nz[1][i]\n",
    "        cv2.arrowedLine(image,\n",
    "                        pt1=tuple(flow_start[y,x]), \n",
    "                        pt2=tuple(flow_end[y,x]),\n",
    "                        color=(0, 255, 255), \n",
    "                        thickness=1, \n",
    "                        tipLength=.2)\n",
    "    return image\n",
    "\n",
    "c(put_optical_flow_arrows_on_image(im1_r_onto_im2_third4,flow))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
